import os
os.environ["BITSANDBYTES_NOWELCOME"] = "1"  # ê²½ê³  ì œê±°
os.environ["USE_BITSANDBYTES"] = "0"       # bitsandbytes ì‚¬ìš© ê¸ˆì§€

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

dataset = "piqa"  # ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì´ë¦„

base_model_name = "meta-llama/Llama-2-7b-hf"
lora_model_name = f"./lora/{dataset}"

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    device_map="cpu",
    torch_dtype="float16"
)

lora_model = PeftModel.from_pretrained(
    base_model,
    lora_model_name,
    device_map="cpu",
    torch_dtype="float16",
    load_in_8bit=False
)

# Merge & Unload
merged_model = lora_model.merge_and_unload()

# ì €ì¥ ê²½ë¡œ
merged_path = f"./merged_models/merged_lora_{dataset}"
merged_model.save_pretrained(merged_path)

# í† í¬ë‚˜ì´ì € ì €ì¥
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.save_pretrained(merged_path)

# í•„ìš” ì—†ëŠ” adapter_config.json ì‚­ì œ
adapter_config_path = os.path.join(merged_path, "adapter_config.json")
if os.path.exists(adapter_config_path):
    os.remove(adapter_config_path)
    print("ğŸ—‘ï¸ adapter_config.json ì‚­ì œ ì™„ë£Œ")

print("âœ… Merge ë° ì •ë¦¬ ì™„ë£Œ!")